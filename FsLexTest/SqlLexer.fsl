
{
// F# code goes here. open's, lookups and other objects referenced by the rules, etc.
module SqlLexer

open System
open Microsoft.FSharp.Text.Lexing
open Parser

let lexeme = LexBuffer<_>.LexemeString
// Notice we've created a few maps, one for keywords and one for operators. 
// While we certainly can define these as rules in our lexer, its generally 
// recommended to have a very small number of rules to avoid a "state explosion".
let keywords =   
    [   
        "SELECT", SELECT;   
        "FROM", FROM;   
        "WHERE", WHERE;   
        "ORDER", ORDER;   
        "BY", BY;   
        "JOIN", JOIN;   
        "INNER", INNER;   
        "LEFT", LEFT;   
        "RIGHT", RIGHT;   
        "ASC", ASC;   
        "DESC", DESC;   
        "AND", AND;   
        "OR", OR;   
        "ON", ON;   
    ] |> Map.ofList   
  
let ops =   
    [   
        "=",    EQ;   
        "<",    LT;   
        "<=",   LE;   
        ">",    GT;   
        ">=",   GE;   
    ] |> Map.ofList   
}   
  
let char        = ['a'-'z' 'A'-'Z']   
let digit       = ['0'-'9']   
let int         = '-'?digit+   
let float       = '-'?digit+ '.' digit+   
let identifier  = char(char|digit|['-' '_' '.'])*   
let whitespace  = [' ' '\t']   
let newline     = "\n\r" | '\n' | '\r'  
let operator    = ">" | ">=" | "<" | "<=" | "="  
  
rule tokenize = parse   
| whitespace    { tokenize lexbuf }   
| newline       { lexbuf.EndPos <- lexbuf.EndPos.NextLine; tokenize lexbuf; }   
| int           { INT(Int32.Parse(lexeme lexbuf)) }   
| float         { FLOAT(Double.Parse(lexeme lexbuf)) }   
| operator      { ops.[lexeme lexbuf] }   
| identifier    { match keywords.TryFind(lexeme lexbuf) with   
                  | Some(token) -> token   
                  | None -> ID(lexeme lexbuf) }   
| ','           { COMMA }   
| eof           { EOF }